{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4927cdda-314e-480f-a991-6a30959b3539",
   "metadata": {},
   "source": [
    "## Checking required tokens for restricted vocab selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a974b71-e695-4c8d-9804-a1645b482426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/navid/miniforge3/envs/rho/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent1 & sent20 -> [1622, 536, 3, 184, 1622, 1755, 1]\n",
      "int12 & int4 -> [16, 17, 2122, 3, 184, 16, 17, 591, 1]\n",
      "sent3 & int8 -> [1622, 519, 3, 184, 16, 17, 927, 1]\n",
      "int7 & sent19 -> [16, 17, 940, 3, 184, 1622, 2294, 1]\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "sample_outputs = [\n",
    "    \"sent1 & sent20\",\n",
    "    \"int12 & int4\",\n",
    "    \"sent3 & int8\",\n",
    "    \"int7 & sent19\"\n",
    "]\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "res = [str(x) for x in tokenizer(sample_outputs)['input_ids']]\n",
    "for s, r in zip(sample_outputs, res):\n",
    "    print(f\"{s} -> {r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4bc33b0b-942d-450c-9c71-620eb8525f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['▁in', 't', '8', '▁', '&', '▁sent', '9', '</s>'],\n",
       " ['▁sent', '35', '▁', '&', '▁in', 't', '47', '</s>'],\n",
       " ['75'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    tokenizer.convert_ids_to_tokens([16, 17, 927, 3, 184, 1622, 1298, 1]),\n",
    "    tokenizer.convert_ids_to_tokens([1622, 2469, 3, 184, 16, 17, 4177, 1]),\n",
    "    tokenizer.convert_ids_to_tokens([3072])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33cfeb1a-1dcd-478b-89a5-fe43367fd727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: 0 -> id: 632\n",
      "token: 1 -> id: 536\n",
      "token: 2 -> id: 357\n",
      "token: 3 -> id: 519\n",
      "token: 4 -> id: 591\n",
      "token: 5 -> id: 755\n",
      "token: 6 -> id: 948\n",
      "token: 7 -> id: 940\n",
      "token: 8 -> id: 927\n",
      "token: 9 -> id: 1298\n",
      "token: 10 -> id: 1714\n",
      "token: 11 -> id: 2596\n",
      "token: 12 -> id: 2122\n",
      "token: 13 -> id: 2368\n",
      "token: 14 -> id: 2534\n",
      "token: 15 -> id: 1808\n",
      "token: 16 -> id: 2938\n",
      "token: 17 -> id: 2517\n",
      "token: 18 -> id: 2606\n",
      "token: 19 -> id: 2294\n",
      "token: 20 -> id: 1755\n",
      "token: 21 -> id: 2658\n",
      "token: 22 -> id: 2884\n",
      "token: 23 -> id: 2773\n",
      "token: 24 -> id: 2266\n",
      "token: 25 -> id: 1828\n",
      "token: 26 -> id: 2688\n",
      "token: 27 -> id: 2555\n",
      "token: 28 -> id: 2577\n",
      "token: 29 -> id: 3166\n",
      "token: 30 -> id: 1458\n",
      "token: 31 -> id: 3341\n",
      "token: 32 -> id: 2668\n",
      "token: 33 -> id: 4201\n",
      "token: 34 -> id: 3710\n",
      "token: 35 -> id: 2469\n",
      "token: 36 -> id: 3420\n",
      "token: 37 -> id: 4118\n",
      "token: 38 -> id: 3747\n",
      "token: 39 -> id: 3288\n",
      "token: 40 -> id: 2445\n",
      "token: 41 -> id: 4853\n",
      "token: 42 -> id: 4165\n",
      "token: 43 -> id: 4906\n",
      "token: 44 -> id: 3628\n",
      "token: 45 -> id: 2128\n",
      "token: 46 -> id: 4448\n",
      "token: 47 -> id: 4177\n",
      "token: 48 -> id: 3707\n",
      "token: 49 -> id: 3647\n",
      "token: 50 -> id: 1752\n",
      "token: 51 -> id: 5553\n",
      "token: 52 -> id: 5373\n",
      "token: 53 -> id: 4867\n",
      "token: 54 -> id: 5062\n",
      "token: 55 -> id: 3769\n",
      "token: 56 -> id: 4834\n",
      "token: 57 -> id: 3436\n",
      "token: 58 -> id: 3449\n",
      "token: 59 -> id: 3390\n",
      "token: 60 -> id: 3328\n",
      "token: 61 -> id: 4241\n",
      "token: 62 -> id: 4056\n",
      "token: 63 -> id: 3891\n",
      "token: 64 -> id: 4389\n",
      "token: 65 -> id: 4122\n",
      "token: 66 -> id: 3539\n",
      "token: 67 -> id: 3708\n",
      "token: 68 -> id: 3651\n",
      "token: 69 -> id: 3951\n",
      "token: 70 -> id: 2518\n",
      "token: 71 -> id: 4450\n",
      "token: 72 -> id: 5865\n",
      "token: 73 -> id: 4552\n",
      "token: 74 -> id: 4581\n",
      "token: 75 -> id: 3072\n",
      "token: 76 -> id: 3959\n",
      "token: 77 -> id: 4013\n",
      "token: 78 -> id: 3940\n",
      "token: 79 -> id: 4440\n",
      "token: 80 -> id: 2079\n",
      "token: 81 -> id: 4959\n",
      "token: 82 -> id: 4613\n",
      "token: 83 -> id: 4591\n",
      "token: 84 -> id: 4608\n",
      "token: 85 -> id: 4433\n",
      "token: 86 -> id: 3840\n",
      "token: 87 -> id: 4225\n",
      "token: 88 -> id: 4060\n",
      "token: 89 -> id: 3914\n",
      "token: 90 -> id: 2394\n",
      "token: 91 -> id: 4729\n",
      "token: 92 -> id: 4508\n",
      "token: 93 -> id: 4271\n",
      "token: 94 -> id: 4240\n",
      "token: 95 -> id: 3301\n",
      "token: 96 -> id: 4314\n",
      "token: 97 -> id: 4327\n",
      "token: 98 -> id: 3916\n",
      "token: 99 -> id: 3264\n"
     ]
    }
   ],
   "source": [
    "for token in [str(i) for i in range(100)]:\n",
    "    print(f\"token: {token} -> id: {tokenizer.get_vocab()[token]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "038ed212-7cc1-47b0-aedb-b34b133bc413",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in [str(i) for i in range(99)]:\n",
    "    assert token in tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f233817-caf8-4742-a7a0-c224ae410fec",
   "metadata": {},
   "source": [
    "## building the vocab we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74a2170e-ed49-4d25-b45f-5cc21cfb996b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '<pad>',\n",
       " 'additional_special_tokens': ['<extra_id_0>',\n",
       "  '<extra_id_1>',\n",
       "  '<extra_id_2>',\n",
       "  '<extra_id_3>',\n",
       "  '<extra_id_4>',\n",
       "  '<extra_id_5>',\n",
       "  '<extra_id_6>',\n",
       "  '<extra_id_7>',\n",
       "  '<extra_id_8>',\n",
       "  '<extra_id_9>',\n",
       "  '<extra_id_10>',\n",
       "  '<extra_id_11>',\n",
       "  '<extra_id_12>',\n",
       "  '<extra_id_13>',\n",
       "  '<extra_id_14>',\n",
       "  '<extra_id_15>',\n",
       "  '<extra_id_16>',\n",
       "  '<extra_id_17>',\n",
       "  '<extra_id_18>',\n",
       "  '<extra_id_19>',\n",
       "  '<extra_id_20>',\n",
       "  '<extra_id_21>',\n",
       "  '<extra_id_22>',\n",
       "  '<extra_id_23>',\n",
       "  '<extra_id_24>',\n",
       "  '<extra_id_25>',\n",
       "  '<extra_id_26>',\n",
       "  '<extra_id_27>',\n",
       "  '<extra_id_28>',\n",
       "  '<extra_id_29>',\n",
       "  '<extra_id_30>',\n",
       "  '<extra_id_31>',\n",
       "  '<extra_id_32>',\n",
       "  '<extra_id_33>',\n",
       "  '<extra_id_34>',\n",
       "  '<extra_id_35>',\n",
       "  '<extra_id_36>',\n",
       "  '<extra_id_37>',\n",
       "  '<extra_id_38>',\n",
       "  '<extra_id_39>',\n",
       "  '<extra_id_40>',\n",
       "  '<extra_id_41>',\n",
       "  '<extra_id_42>',\n",
       "  '<extra_id_43>',\n",
       "  '<extra_id_44>',\n",
       "  '<extra_id_45>',\n",
       "  '<extra_id_46>',\n",
       "  '<extra_id_47>',\n",
       "  '<extra_id_48>',\n",
       "  '<extra_id_49>',\n",
       "  '<extra_id_50>',\n",
       "  '<extra_id_51>',\n",
       "  '<extra_id_52>',\n",
       "  '<extra_id_53>',\n",
       "  '<extra_id_54>',\n",
       "  '<extra_id_55>',\n",
       "  '<extra_id_56>',\n",
       "  '<extra_id_57>',\n",
       "  '<extra_id_58>',\n",
       "  '<extra_id_59>',\n",
       "  '<extra_id_60>',\n",
       "  '<extra_id_61>',\n",
       "  '<extra_id_62>',\n",
       "  '<extra_id_63>',\n",
       "  '<extra_id_64>',\n",
       "  '<extra_id_65>',\n",
       "  '<extra_id_66>',\n",
       "  '<extra_id_67>',\n",
       "  '<extra_id_68>',\n",
       "  '<extra_id_69>',\n",
       "  '<extra_id_70>',\n",
       "  '<extra_id_71>',\n",
       "  '<extra_id_72>',\n",
       "  '<extra_id_73>',\n",
       "  '<extra_id_74>',\n",
       "  '<extra_id_75>',\n",
       "  '<extra_id_76>',\n",
       "  '<extra_id_77>',\n",
       "  '<extra_id_78>',\n",
       "  '<extra_id_79>',\n",
       "  '<extra_id_80>',\n",
       "  '<extra_id_81>',\n",
       "  '<extra_id_82>',\n",
       "  '<extra_id_83>',\n",
       "  '<extra_id_84>',\n",
       "  '<extra_id_85>',\n",
       "  '<extra_id_86>',\n",
       "  '<extra_id_87>',\n",
       "  '<extra_id_88>',\n",
       "  '<extra_id_89>',\n",
       "  '<extra_id_90>',\n",
       "  '<extra_id_91>',\n",
       "  '<extra_id_92>',\n",
       "  '<extra_id_93>',\n",
       "  '<extra_id_94>',\n",
       "  '<extra_id_95>',\n",
       "  '<extra_id_96>',\n",
       "  '<extra_id_97>',\n",
       "  '<extra_id_98>',\n",
       "  '<extra_id_99>']}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "611522e4-1723-433c-8b48-6ee57408bf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "restricted_vocab = set()\n",
    "for tokens in tokenizer(sample_outputs)['input_ids']:\n",
    "    restricted_vocab.update(tokens)\n",
    "\n",
    "restricted_vocab.update([tokenizer.get_vocab()[i] for i in [str(x) for x in range(100)]])\n",
    "restricted_vocab.update([tokenizer.eos_token_id])\n",
    "restricted_vocab_list = list(restricted_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6b10f3fb-c7d5-4fa4-9b18-03703bc07e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  107\n"
     ]
    }
   ],
   "source": [
    "orig_to_restricted_mapping = {}\n",
    "\n",
    "for t in restricted_vocab_list:\n",
    "    orig_to_restricted_mapping[t] = len(orig_to_restricted_mapping)+1\n",
    "\n",
    "orig_to_restricted_mapping[tokenizer.pad_token_id] = 0\n",
    "print(\"vocab size: \", len(orig_to_restricted_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b7a37b0c-62f3-43a9-a58e-f3524da44919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3072: 1,\n",
       " 1: 2,\n",
       " 4608: 3,\n",
       " 3: 4,\n",
       " 4613: 5,\n",
       " 519: 6,\n",
       " 16: 7,\n",
       " 17: 8,\n",
       " 2577: 9,\n",
       " 4118: 10,\n",
       " 536: 11,\n",
       " 4122: 12,\n",
       " 2079: 13,\n",
       " 2596: 14,\n",
       " 3628: 15,\n",
       " 2606: 16,\n",
       " 3647: 17,\n",
       " 3651: 18,\n",
       " 4165: 19,\n",
       " 2122: 20,\n",
       " 591: 21,\n",
       " 2128: 22,\n",
       " 4177: 23,\n",
       " 1622: 24,\n",
       " 3166: 25,\n",
       " 2658: 26,\n",
       " 4201: 27,\n",
       " 2668: 28,\n",
       " 632: 29,\n",
       " 4729: 30,\n",
       " 3707: 31,\n",
       " 3708: 32,\n",
       " 3710: 33,\n",
       " 2688: 34,\n",
       " 4225: 35,\n",
       " 4240: 36,\n",
       " 4241: 37,\n",
       " 3747: 38,\n",
       " 4271: 39,\n",
       " 1714: 40,\n",
       " 184: 41,\n",
       " 3769: 42,\n",
       " 3264: 43,\n",
       " 2773: 44,\n",
       " 3288: 45,\n",
       " 1752: 46,\n",
       " 2266: 47,\n",
       " 1755: 48,\n",
       " 4314: 49,\n",
       " 4834: 50,\n",
       " 3301: 51,\n",
       " 4327: 52,\n",
       " 5865: 53,\n",
       " 755: 54,\n",
       " 4853: 55,\n",
       " 2294: 56,\n",
       " 5373: 57,\n",
       " 3328: 58,\n",
       " 3840: 59,\n",
       " 4867: 60,\n",
       " 3341: 61,\n",
       " 1808: 62,\n",
       " 1298: 63,\n",
       " 1828: 64,\n",
       " 4389: 65,\n",
       " 4906: 66,\n",
       " 3891: 67,\n",
       " 3390: 68,\n",
       " 2368: 69,\n",
       " 2884: 70,\n",
       " 3914: 71,\n",
       " 3916: 72,\n",
       " 4433: 73,\n",
       " 4440: 74,\n",
       " 2394: 75,\n",
       " 3420: 76,\n",
       " 4959: 77,\n",
       " 4448: 78,\n",
       " 4450: 79,\n",
       " 3940: 80,\n",
       " 357: 81,\n",
       " 3436: 82,\n",
       " 3951: 83,\n",
       " 3959: 84,\n",
       " 3449: 85,\n",
       " 2938: 86,\n",
       " 2445: 87,\n",
       " 4508: 88,\n",
       " 927: 89,\n",
       " 2469: 90,\n",
       " 940: 91,\n",
       " 4013: 92,\n",
       " 5553: 93,\n",
       " 1458: 94,\n",
       " 948: 95,\n",
       " 5062: 96,\n",
       " 4552: 97,\n",
       " 3539: 98,\n",
       " 2517: 99,\n",
       " 2518: 100,\n",
       " 4056: 101,\n",
       " 4060: 102,\n",
       " 4581: 103,\n",
       " 2534: 104,\n",
       " 4591: 105,\n",
       " 2555: 106,\n",
       " 0: 0}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_to_restricted_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57418de2-9ae1-4576-86f8-192efd0da91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('modeling/restricted_t5_vocab.json', 'w') as f:\n",
    "    json.dump(orig_to_restricted_mapping, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
